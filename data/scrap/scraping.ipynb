{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages: 100%|██████████| 290/290 [04:16<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 11600 links:\n",
      "- Pickle file: saved_files/saved_links.p\n",
      "- Text file: saved_files/saved_links.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "root_url = \"https://gameknot.com\"\n",
    "\n",
    "saved_links = []\n",
    "\n",
    "os.makedirs(\"saved_files\", exist_ok=True)\n",
    "\n",
    "text_file_path = \"saved_files/saved_links.txt\"\n",
    "\n",
    "for page_index in tqdm(range(290), desc=\"Processing pages\"):\n",
    "    page_url = f\"https://gameknot.com/list_annotated.pl?u=all&c=0&sb=0&rm=0&rn=0&rx=9999&sr=0&p={page_index}\"\n",
    "\n",
    "    r = requests.get(page_url)\n",
    "    \n",
    "    if r.status_code == 200:\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        \n",
    "        for elem in soup.find_all('tr', class_=[\"evn_list\", \"odd_list\"]):\n",
    "            list_of_links = elem.find_all('a')\n",
    "            \n",
    "            if len(list_of_links) > 1:\n",
    "                href = list_of_links[1].get('href')\n",
    "                if href:\n",
    "                    full_link = root_url + href\n",
    "                    saved_links.append(full_link)\n",
    "        \n",
    "        #print(f\"Processed page {page_index}\")\n",
    "    else:\n",
    "        #print(f\"Failed to fetch page {page_index} with status code {r.status_code}\")\n",
    "        break\n",
    "\n",
    "pickle_file_path = \"saved_files/saved_links.p\"\n",
    "with open(pickle_file_path, \"wb\") as pickle_file:\n",
    "    pickle.dump(saved_links, pickle_file)\n",
    "\n",
    "with open(text_file_path, \"w\") as text_file:\n",
    "    for link in saved_links:\n",
    "        text_file.write(link + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(saved_links)} links:\")\n",
    "print(f\"- Pickle file: {pickle_file_path}\")\n",
    "print(f\"- Text file: {text_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data for 0 entries.\n",
      "All data saved in: saved_files/commentary.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "links_file_path = \"saved_files/saved_links.txt\"\n",
    "\n",
    "output_dir_path = \"saved_files\"\n",
    "os.makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "output_file_path = os.path.join(output_dir_path, \"commentary.txt\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "with open(links_file_path, \"r\") as file:\n",
    "    links = [line.strip() for line in file.readlines()]\n",
    "\n",
    "with open(output_file_path,'w') as f:\n",
    "    for idx, link in enumerate(tqdm(links[:10]), start=1):#Sample\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            \n",
    "            if r.status_code == 200:\n",
    "                soup = BeautifulSoup(r.content, 'html.parser')\n",
    "                \n",
    "                rows = soup.find_all('tr')\n",
    "                \n",
    "                for row in rows:\n",
    "                    move_td = row.find('td', rowspan='2', style=lambda value: value and 'vertical-align: top' in value)\n",
    "                    comment_td = row.find('td', style=\"vertical-align: top;\")\n",
    "                    \n",
    "                    if move_td and comment_td:\n",
    "                        move_text = move_td.get_text(separator=' ', strip=True)\n",
    "                        move_text = ' '.join(move_text.split())\n",
    "                        \n",
    "                        comment_text = comment_td.get_text(separator=' ', strip=True)\n",
    "                        comment_text = ' '.join(comment_text.split())\n",
    "\n",
    "                        f.write(f\"{move_text} <sep> {comment_text}\\n\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"Failed to fetch link {idx} with status code {r.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while processing link {idx}: {e}\")\n",
    "\n",
    "print(f\"Scraped data for {len(all_data)} entries.\")\n",
    "print(f\"All data saved in: {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
